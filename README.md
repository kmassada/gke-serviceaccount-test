# GKE with serviceaccount

## Goals

- create a service account `NODE_SA_NAME` instead of using scopes for creating GKE cluster
- create a service account `CICD_SA_NAME` used for deploying applications my prefference `skaffold`
- create a service account `APP_SA_NAME` that interracts with GCP

## Create Node's Service Account

```shell
export NODE_SA_NAME=gke-node-sa

gcloud iam service-accounts create $NODE_SA_NAME --display-name "Node Service Account"
export NODE_SA_EMAIL=`gcloud iam service-accounts list --format='value(email)' --filter='displayName:Node Service Account'`

export PROJECT=`gcloud config get-value project`

gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:${NODE_SA_EMAIL} --role=roles/monitoring.metricWriter
gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:${NODE_SA_EMAIL} --role=roles/monitoring.viewer
gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:${NODE_SA_EMAIL} --role=roles/logging.logWriter

gcloud config set container/new_scopes_behavior true
```

## Create Cluster

```shell
export CLUSTER_NAME=serviceaccount-test

gcloud beta container clusters create serviceaccount-test \
  --service-account=$NODE_SA_EMAIL \
  --zone=$ZONE \
  --cluster-version=$VERSION
```

## Create CI/CD's Service Account

```shell
# Create service account
export CICD_SA_NAME=gke-cicd-sa
gcloud iam service-accounts create $CICD_SA_NAME --display-name "CI/CD Service Account"
export CICD_SA_EMAIL=`gcloud iam service-accounts list --format='value(email)' --filter='displayName:CI/CD Service Account'`

# Bind service account policy
export PROJECT=`gcloud config get-value project`

gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:${CICD_SA_EMAIL} --role=roles/container.developer

# Create service account key
gcloud iam service-accounts keys create \
    /home/$USER/key.json \
    --iam-account $CICD_SA_EMAIL
```

NOTE: Export created key `key.json` to CICD's node and activate it.

```shell
gcloud auth activate-service-account $CICD_SA_EMAIL --key-file=key.json
```

`get-credentials` appends to the `kubeconfig` file, the context for the GKE master we want to auth against, it's formatted like this `gke_$PROJECT_$ZONE_$CLUSTER_NAME`

```shell
GOOGLE_APPLICATION_CREDENTIALS="/home/$USER/key.json" gcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE --project $PROJECT
```

### Token Alternative

had a use case where I needed to use tokens. instead of generated credentials, skip to [Testing Context](#testing-context)

```shell
# Generate token from key
export GOOGLE_APPLICATION_CREDENTIALS="/home/$USER/key.json"
gcloud beta auth application-default print-access-token > /home/$USER/token
```

add the token to your `/home/$USER/.kube/config`, remove auth-provider from your config file

```diff
18,26c18
<                 "auth-provider": {
<                     "name": "gcp",
<                     "config": {
<                         "cmd-args": "config config-helper --format=json",
<                         "cmd-path": "/google/google-cloud-sdk/bin/gcloud",
<                         "expiry-key": "{.credential.token_expiry}",
<                         "token-key": "{.credential.access_token}"
<                     }
<                 }
---
>                 "token": "<TOKEN>"
```

Then add the content of your token, `$context_user` in this case, is the user that was generated by `get-context`

```shell
kubectl config set-credentials $context_user --token=$(cat /home/$USER/token)
```

### Testing Context

context was generated using `gcloud container clusters`

```shell
GOOGLE_APPLICATION_CREDENTIALS="/home/$USER/key.json" gcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE --project $PROJECT
```

there's no need to activate the context since `gcloud container clusters get-credentials` did it for you, but just to be explicit, list context avaialable to you first

```shell
kubectl config get-contexts
```

activate the context you want this way, where `$context_name` is the context generated by get credentials, at the time of this tutorial, it's formatted like this `gke_$PROJECT_$ZONE_$CLUSTER_NAME` using variables already set.

```shell
kubectl config use-context $context_name
```

use the token to delete a random item

```shell
kubectl get pods
kubectl delete pods/$podname --namespace $namespace
```

verify in stackdriver

```shell
resource.type="k8s_cluster"
resource.labels.location="$ZONE"
resource.labels.cluster_name="$CLUSTER_NAME"
protoPayload.methodName:"delete"
protoPayload.resourceName="core/v1/namespaces/$namespace/pods/$podname"
```

## Run Workload

In shell set this variable

```shell
APPLICATION=web-app
```

this part of the tutorial is assuming workload is pulling an image from gcr inside the same project `gcr.io/$PROJECT/$PREFIX/$APPLICATION`

```shell
gcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE --project $PROJECT

kubectl run $APPLICATION --image=gcr.io/$PROJECT/$PREFIX/$APPLICATION
```

### Workload Fails (ImagePullBackOff)

```shell
POD_NAME=`kubectl get pods -o jsonpath='{.items[?(@.metadata.labels.run=="$APPLICATION")].metadata.name}'`

$ kubectl describe pod $POD_NAME
...
Events:
  Type     Reason                 Age               From                                                          Message
  ----     ------                 ----              ----                                                          -------
  Normal   Scheduled              7m                default-scheduler                                             Successfully assigned $APPLICATION-764784b488-kcgvv to $CLUSTER_NAME-default-pool-a262a520-7dw5
  Normal   SuccessfulMountVolume  7m                kubelet, $CLUSTER_NAME-default-pool-a262a520-7dw5  MountVolume.SetUp succeeded for volume "default-token-t8sg8"
  Normal   Pulling                5m (x4 over 7m)   kubelet, $CLUSTER_NAME-default-pool-a262a520-7dw5  pulling image "gcr.io/$PROJECT/$PREFIX/$APPLICATION"
  Warning  Failed                 5m (x4 over 7m)   kubelet, $CLUSTER_NAME-default-pool-a262a520-7dw5  Failed to pull image "gcr.io/$PROJECT/$PREFIX/$APPLICATION": rpc er
ror: code = Unknown desc = Error response from daemon: repository gcr.io/$PROJECT/$PREFIX/$APPLICATION not found: does not exist or no pull access
  Warning  Failed                 5m (x4 over 7m)   kubelet, $CLUSTER_NAME-default-pool-a262a520-7dw5  Error: ErrImagePull
  Normal   BackOff                5m (x6 over 7m)   kubelet, $CLUSTER_NAME-default-pool-a262a520-7dw5  Back-off pulling image "gcr.io/$PROJECT/$PREFIX/$APPLICATION"
  Warning  Failed                 2m (x20 over 7m)  kubelet, $CLUSTER_NAME-default-pool-a262a520-7dw5  Error: ImagePullBackOff
```

### Setting right pull permissions

NOTE: `CICD_SA` was not configured to be able to set iam permissions, these steps are done via the same admin account that created `CICD_SA`

<!--
here's how I wish it worked
```shell
BUCKET_PATH=artifacts.$PROJECT.appspot.com/containers/repositories/library/$PREFIX/
gsutil acl ch -r -u $NODE_SA_EMAIL:R gs://$BUCKET_PATH
```-->

<!--
This is not recommended
gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:${NODE_SA_EMAIL} --role=roles/storage.objectViewer
-->

```shell
BUCKET_NAME=artifacts.$PROJECT.appspot.com/
gsutil iam ch serviceAccount:$NODE_SA_EMAIL:objectViewer gs://$BUCKET_NAME

POD_NAME=`kubectl get pods -o jsonpath='{.items[?(@.metadata.labels.run=="$APPLICATION")].metadata.name}'`

kubectl describe pod $POD_NAME
```

## Create Application's Service Account

there are 2 schools of thoughts here, RBAC is more granular when it comes to permissions to cluster resources, see [kmassada/gke-rbac-test](https://github.com/kmassada/gke-rbac-test). Essentially it mounts your service account into a namespace and allows for kubectl access via the default token that is mounted.

The second school of thought is to use `gcloud container get-clusters`, like in the CICD example.

However For the sake of this exercise, we want permissions to access other GCP resources. example:

- [Compute Engine API v1](https://cloud.google.com/compute/docs/reference/rest/v1/)

```shell
# Create service account
export APP_SA_NAME=gke-$APPLICATION-sa
gcloud iam service-accounts create $APP_SA_NAME --display-name "GKE $APPLICATION Application Service Account"
export APP_SA_EMAIL=`gcloud iam service-accounts list --format='value(email)' --filter="displayName:GKE $APPLICATION Application Service Account"`

# Bind service account policy
export PROJECT=`gcloud config get-value project`

gcloud projects add-iam-policy-binding $PROJECT --member=serviceAccount:${APP_SA_EMAIL} --role=roles/compute.viewer

# Create service account key and activate it
gcloud iam service-accounts keys create \
    /home/$USER/key.json \
    --iam-account $APP_SA_EMAIL
```

### Configure application

```shell
kubectl create configmap project-id --from-literal "project-id=${PROJECT}"
kubectl create configmap $APPLICATION-zone --from-literal "$APPLICATION-zone=${ZONE}"
kubectl create configmap $APPLICATION-sa --from-literal "sa-email=${APP_SA_EMAIL}"
kubectl create secret generic $APPLICATION --from-file key.json
```

`deploymnent.yaml` adds 3 env variables

- GOOGLE_APPLICATION_CREDENTIALS
- PROJECT_ID
- APP_SA_EMAIL
- ZONE

those environment variables are local to the container

```shell
envsubst < deployment.template.yaml > deployment.yaml
kubectl apply -f deployment.yaml
```

```shell
POD_NAME=`kubectl get pods -o jsonpath='{.items[?(@.metadata.labels.run=="$APPLICATION")].metadata.name}'`

kubectl describe pod $POD_NAME
```

drop into shell

```shell
kubectl exec -it  $POD_NAME -- bash
```

in our pod, we install google-cloud-sdk

```shell
apt-get update -qy && apt-get -qy install curl dnsutils gnupg lsb-release
export CLOUD_SDK_REPO="cloud-sdk-$(lsb_release -c -s)"
echo "deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
apt-get update && apt-get install -qy google-cloud-sdk
```

now we see our container is active using NODE_SA

```console
gcloud auth list
                   Credentialed Accounts
ACTIVE  ACCOUNT
*       $NODE_SA_NAME@$PROJECT.iam.gserviceaccount.com
```

we force instead our container to auth with APP_SA

```shell
gcloud auth activate-service-account $APP_SA_EMAIL --key-file=$GOOGLE_APPLICATION_CREDENTIALS
```

here so we can now test, we've added the role `roles/compute.viewer` this user can list nodes

```console
# gcloud compute instances list
NAME                                       ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
builder                                    $ZONE  n1-standard-1 ......
```

but can't list clusters in a zone.

```console
# gcloud container clusters list --zone $ZONE
ERROR: (gcloud.container.clusters.list) ResponseError: code=403, message=Required "container.clusters.list" permission for "projects/$PROJECT"
```

In the auth list we can see the proper service account is selected

```console
# gcloud auth list
                     Credentialed Accounts
ACTIVE  ACCOUNT
        $NODE_SA_NAME@makz-support-eap.iam.gserviceaccount.com
*       $APP_SA_NAME@makz-support-eap.iam.gserviceaccount.com

To set the active account, run:
    $ gcloud config set account `ACCOUNT`
```
